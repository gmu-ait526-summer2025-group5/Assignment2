{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404de6b7",
   "metadata": {},
   "source": [
    "Group Programming Assignment Two-QA System <br>\n",
    "Kori Fogle, Michaela Herrick, Jackson Holland, Ishaan Indoori <br>\n",
    "AIT 526 <br>\n",
    "6/18/2025 <br>\n",
    "\n",
    "<b> Problem to be Solved </b> <br>\n",
    "The goal of this program is to create a question answering system executable from the command line. These questions must begin with \"who,\" \"what,\" \"when,\" or \"where\" and must return a portion of the question back to the user along with the answer. Unclear questions or questions to which answers cannot be found must be addressed. A log file must be created within the program, and users must be able to exit the chat.\n",
    " <br>\n",
    "<b> Algorithm and Flow</b><br>\n",
    "Our program utilizes a class that contains several functions to accomplish the above goal. First, we loaded all relevant packages. Particularly, this program will use a Wikipedia API to return answers back to a user. We then initialize a log file, as well as create a way for the user to exit. These questions must begin with \"who,\" \"what,\" \"when,\" or \"where.\" The program uses regular expressions and if statements to parse user input, then return answers that rephrase part of the user's original question along with the answer. The user’s input is then categorized into one five types of questions- a who question, a what question, a where question, a when question, or an unclear/ unanswerable question. These questions, except for unclear/ unanswerable questions, are then queried using the Wikipedia API. An answer is then returned to the user. The program also alerts users when it is given a question it cannot answer with a phrase such as \"I'm sorry, I don't quite know the answer.\" Further, users can exit the program using the word \"exit.\" This program also creates a log file that can be referenced after program ends to review input and output. Finally, error handling is included to address specific errors, such as pages not loading, or too many possible answers being returned.\n",
    "<br>\n",
    "<b> Example of Input and Output</b><br>\n",
    "\n",
    "<b> Usage Instructions</b><br>\n",
    "<oi>\n",
    "<li>Execute the program</li>\n",
    "<li>You'll be asked to input a name for the log file. Name this file as you would any other file. It will be accessible in the same working directory as this program.</li>\n",
    "<li>Instructions will prompt you to start all questions with who, what, when, or where.</li>\n",
    "<li>Ask a question and wait for the response</li>\n",
    "<li>Stop asking questions by typing the word \"exit.\"</li>\n",
    "</ol><br>\n",
    "\n",
    "<b> References</b><br>\n",
    "Anon. 2024. “Python Regex Cheat Sheet.” GeeksforGeeks. Retrieved May 31, 2025 (https://www.geeksforgeeks.org/python-regex-cheat-sheet/).<br>\n",
    "Dib, Firas. n.d. “Regex101 - Online Regex Editor and Debugger.” @Regex101.(https://regex101.com/).<br>\n",
    "Gadiraju, Sai Surya. n.d. “Program Assignment 2 Demo Video.”<br>\n",
    "Jurafsky, Daniel, and James H. Martin. 2025. Speech and Language Processing (3rd Ed. Draft).<br>\n",
    "Liao, Duoduo. n.d. \"Tips and Hints- QA System Programming Assignment-2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e85835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant packages and en_core_web_sm\n",
    "import sys\n",
    "import wikipedia\n",
    "import wikipedia.exceptions as wiki_exceptions\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b483d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** This is a QA system. I will try to answer questions that start with Who, What, When, and Where. Type Exit to quit. ***\n",
      "Trying to search Wikipedia for the question: Rosa Parks\n",
      "Unfortunately I could not find a page on that topic.\n",
      "Trying to search Wikipedia for the question: george washington\n",
      "=> George Washington was born on February 22, 1732.\n",
      "Trying to search Wikipedia for the question: Thomas Jefferson\n",
      "=> Thomas Jefferson was born on April 13, 1743.\n",
      "Thank you, goodbye\n",
      "Log is saved.\n"
     ]
    }
   ],
   "source": [
    "# This version merges your enhanced code into your team's QA_System class\n",
    "# while preserving all of their original structure and functionality.\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import wikipedia\n",
    "from wikipedia import exceptions as wiki_exceptions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class QA_System:\n",
    "    def __init__(self, logfile): \n",
    "        # Initialize the QA system with a log file for storing Q&A history\n",
    "        self.logfile = logfile\n",
    "\n",
    "    def run(self):\n",
    "        # Main loop for interactive Q&A\n",
    "        print(\"*** This is a QA system. I will try to answer questions that start with Who, What, When, and Where. Type Exit to quit. ***\")\n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"*?>\").strip()\n",
    "                if question.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
    "                    print(\"Thank you, goodbye\")\n",
    "                    break\n",
    "                self.answer_question(question)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                continue\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        # Determine question type and extract a query to search\n",
    "        doc = nlp(question)\n",
    "        question_type = self.identify_question_type(question)\n",
    "        if question_type is None:\n",
    "            print(\"I'm sorry I don't quite know the answer to this question.\")\n",
    "            self.log_question(question, \"n/a\")\n",
    "            return \n",
    "        refined_query = self.extract_context(question)\n",
    "        if not refined_query:\n",
    "            refined_query = self.extract_dynamic_entity(doc, question_type)\n",
    "        if refined_query: \n",
    "            print(f\"Trying to search Wikipedia for the question: {refined_query}\")\n",
    "            self.search_wikipedia(refined_query, question_type, question)\n",
    "        else:\n",
    "            print(\"I'm sorry, but I was unable to find an answer. Make sure you've phrased your question correctly.\")\n",
    "\n",
    "    def identify_question_type(self, question):\n",
    "        # Categorize the question by its starting keyword\n",
    "        question_lower = question.lower()\n",
    "        if question_lower.startswith(\"who\"):\n",
    "            return \"Who\"\n",
    "        elif question_lower.startswith(\"what\"):\n",
    "            return \"What\"\n",
    "        elif question_lower.startswith(\"when\"):\n",
    "            return \"When\"\n",
    "        elif question_lower.startswith(\"where\"):\n",
    "            return \"Where\"\n",
    "        return None\n",
    "\n",
    "    def extract_context(self, question):        \n",
    "        # Regex patterns to extract the relevant topic from the question\n",
    "        patterns = [\n",
    "            r'Who (?:is|was|are)? (.+)',\n",
    "            r'Who (made|makes|created|invented|discovered|wrote) (.+)',\n",
    "            r'Who (owns|founded|leads|led) (.+)',\n",
    "            r'What (?:is|was)? (.+)',\n",
    "            r'what (?:is|was)? ( .* ) Age',\n",
    "            r'When (?:is|was) (.+) born',\n",
    "            r'When (?:is|was) (.+) birthday',\n",
    "            r'When did (.+)',\n",
    "            r'When (.+) Born',\n",
    "            r'When (.+) Birthday',\n",
    "            r'Where (?:is|was|are|did)? (.+)',\n",
    "            r'Where (.+)',\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.match(pattern, question, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(len(match.groups())).strip()\n",
    "        return None\n",
    "\n",
    "    def extract_dynamic_entity(self, doc, question_type):\n",
    "        # Try to dynamically extract named entities using spaCy\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"DATE\"}]\n",
    "        if entities:\n",
    "            return \" \".join(entities)\n",
    "        # If no named entities are found, fallback to a cleaned version of the full input text\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', doc.text).strip().lower()\n",
    "\n",
    "    def fuzzy_match(self, subject, results):\n",
    "        # Use string similarity to select the most relevant Wikipedia title\n",
    "        # Fuzzy matching improves resilience to typos or Wikipedia title mismatches\n",
    "        def similarity(a, b):\n",
    "            return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "        ranked = sorted(results, key=lambda title: similarity(subject, title), reverse=True)\n",
    "        return ranked[0] if ranked else None\n",
    "\n",
    "    def search_wikipedia(self, query, question_type, question):\n",
    "        # Search Wikipedia for the topic and extract relevant content\n",
    "        try:\n",
    "            search_results = wikipedia.search(query)\n",
    "            if not search_results:\n",
    "                print(\"I am sorry I cannot seem to find the answer.\")\n",
    "                self.log_question(question, \"n/a\")\n",
    "                return\n",
    "\n",
    "            chosen_title = self.fuzzy_match(query, search_results) or search_results[0]\n",
    "            try:\n",
    "                page = wikipedia.page(chosen_title)\n",
    "            except wiki_exceptions.DisambiguationError:\n",
    "                print(\"I'm sorry, I don't understand the question.\")\n",
    "                self.log_question(question, \"ambiguous\")\n",
    "                return\n",
    "\n",
    "            # Tokenize summary for concise and high-level info (improves answer relevance)\n",
    "            summary_sentences = sent_tokenize(page.summary)\n",
    "            # Tokenize beginning of full content for supporting context\n",
    "            content_sentences = sent_tokenize(page.content)\n",
    "            # Combine summary and first 25 content sentences\n",
    "            combined_sentences = summary_sentences + content_sentences[:25]\n",
    "\n",
    "            meaningful_summary = self.summarize_text(combined_sentences, question_type, query, question)\n",
    "            if meaningful_summary:\n",
    "                print(f\"=> {meaningful_summary}\")\n",
    "                self.log_question(question, meaningful_summary)\n",
    "            else:\n",
    "                print(\"I am sorry I cannot seem to find the answer.\")\n",
    "                self.log_question(question, \"N/A\")\n",
    "\n",
    "        except wiki_exceptions.PageError:\n",
    "            print(\"Unfortunately I could not find a page on that topic.\")\n",
    "            self.log_question(question, \"no result\")\n",
    "\n",
    "        except wiki_exceptions.HTTPTimeoutError:\n",
    "            print(\"There's a network error, check your internet connection and try again.\")\n",
    "            self.log_question(question, \"timeout\")\n",
    "\n",
    "    def summarize_text(self, sentences, question_type, query, full_question):\n",
    "        # Analyze and extract a sentence that best answers the question based on its type\n",
    "        # Consider breaking these blocks into helpers to reduce complexity\n",
    "        doc_query = nlp(query)\n",
    "        clean_name = self.clean_display_name(query)\n",
    "\n",
    "        if question_type == \"Who\":\n",
    "            for sentence in sentences:\n",
    "                if query.lower() in sentence.lower() and (\"is\" in sentence or \"was\" in sentence):\n",
    "                    return sentence\n",
    "            return sentences[0] if sentences else None\n",
    "\n",
    "        elif question_type == \"What\":\n",
    "            return sentences[0] if sentences else None\n",
    "\n",
    "        elif question_type == \"When\":\n",
    "            doc = nlp(query)\n",
    "            is_person = any(ent.label_ == \"PERSON\" for ent in doc.ents)\n",
    "            is_death = bool(re.search(r'\\b(die|death|passed away|dead)\\b', query, re.IGNORECASE))\n",
    "\n",
    "            full_date_pattern = r'([A-Z][a-z]+ \\d{1,2}, \\d{4})'\n",
    "\n",
    "            for sentence in sentences:\n",
    "                if is_death:\n",
    "                    match = re.search(r'died on ' + full_date_pattern, sentence)\n",
    "                    if match:\n",
    "                        return f\"{clean_name} died on {match.group(1)}.\"\n",
    "                    match_alt = re.search(r'on ' + full_date_pattern + r' and died', sentence)\n",
    "                    if match_alt:\n",
    "                        return f\"{clean_name} died on {match_alt.group(1)}.\"\n",
    "\n",
    "                if is_person:\n",
    "                    match = re.search(r'born on ' + full_date_pattern, sentence)\n",
    "                    if match:\n",
    "                        return f\"{clean_name} was born on {match.group(1)}.\"\n",
    "                    match_alt = re.search(r'on ' + full_date_pattern + r' and born', sentence)\n",
    "                    if match_alt:\n",
    "                        return f\"{clean_name} was born on {match_alt.group(1)}.\"\n",
    "\n",
    "                if not is_person:\n",
    "                    match = re.search(\n",
    "                        r'(?:started|began|occurred|established|founded|created|formed|launched)(?: in| on)? ' + full_date_pattern,\n",
    "                        sentence, re.IGNORECASE\n",
    "                    )\n",
    "                    if match:\n",
    "                        return f\"{clean_name} was established on {match.group(1)}.\"\n",
    "\n",
    "            # ✅ Extra Credit: Fallback if exact sentence isn't found but a DATE exists\n",
    "            for sent in sentences:\n",
    "                sent_doc = nlp(sent)\n",
    "                for ent in sent_doc.ents:\n",
    "                    if ent.label_ == \"DATE\" and re.search(r'\\d{4}', ent.text):\n",
    "                        # Heuristic for human names vs other topics\n",
    "                        if is_death:\n",
    "                            return f\"While the exact date is unclear, it appears {clean_name} died in {ent.text}.\"\n",
    "                        elif is_person:\n",
    "                            return f\"While the exact date is unclear, it appears {clean_name} was born in {ent.text}.\"\n",
    "                        else:\n",
    "                            return f\"While the exact date is unclear, it appears {clean_name} was established in {ent.text}.\"\n",
    "\n",
    "            return \"Date or time information not found.\"\n",
    "\n",
    "        elif question_type == \"Where\":\n",
    "            for sentence in sentences:\n",
    "                if \"GPE\" in [ent.label_ for ent in nlp(sentence).ents]:\n",
    "                    return sentence\n",
    "            return \"Location information not found.\"\n",
    "\n",
    "    def log_question(self, question, answer):\n",
    "        # Append question and answer to the log file\n",
    "        with open(self.logfile, 'a', encoding='utf-8') as log:\n",
    "            log.write(f\"Question: {question}\\n\")\n",
    "            log.write(f\"Answer: {answer}\\n\\n\")\n",
    "\n",
    "    def clean_display_name(self, query):\n",
    "        # Construct clean names like 'Albert Einstein' by stripping auxiliary verbs and keywords\n",
    "        query = re.sub(r'\\b(when|was|did|is|born|died|created|founded|started|begin|occurred|took place|happen(ed)?)\\b', '', query, flags=re.IGNORECASE)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip(\"? \").strip()\n",
    "        return query.title()\n",
    "\n",
    "def main():\n",
    "    log_filename = input(\"Enter the name of the log file: \").strip()\n",
    "    try:\n",
    "        qa_system = QA_System(log_filename)\n",
    "        qa_system.run()\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    finally:\n",
    "        print(\"Log is saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce004e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
